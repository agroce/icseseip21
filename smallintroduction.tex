\section{Introduction}

Bitcoin~\cite{nakamoto2008bitcoin} is the most popular cryptocurrency, and one of the most visible (if controversial) ``new'' systems based on software to rise to prominence in the last decade.  As we write, while volatile, Bitcoin consistently has a market cap of over half a trillion dollars since January of 2021.  Due to its distributed, decentralized nature, Bitcoin in some sense is the sum of the operations of the code executed by many independent Bitcoin nodes, especially nodes that mine cryptocurrency.  Bitcoin Core (\url{https://github.com/Bitcoin/Bitcoin}) is by far the most popular implementation, and serves as a reference for all other implementations.  To a significant degree, the code of Bitcoin Core \emph{is} Bitcoin.  The main Bitcoin Core repo on GitHub has over 57,000 stars, and has been forked more than 30,000 times.

Because of its fame and the high monetary value of Bitcoins, the Bitcoin protocol and its implementations are a high-value target for hackers (or even nation states interested in controlling cryptocurrency developments).  Therefore, testing the code is of paramount importance, including extensive functional tests and aggressive \emph{fuzzing}.  This paper describes a focused effort to identify weaknesses in, and improve, the fuzzing of Bitcoin Core.

%\section{Initial Contact and the Problem of Saturation}

Chaincode Labs (\url{https://chaincode.com/}) is a private R\&D center based in Manhattan that exists solely to support and develop Bitcoin.  In March of 2021, Adam Jonas, the head of special projects at Chaincode, contacted the first author to discuss determining a strategy to improve the fuzzing of Bitcoin Core.  It seemed that the fuzzing was ``stuck'': neither code coverage nor found bugs were increasing with additional fuzzing time.  After some discussion, an 80 hour effort was determined as a reasonable scope for an external, research-oriented, look at the fuzzing effort.

%Before that effort, conducted over the summer of 2021, began, the problem of saturation resolved itself.  Nonetheless, the issue that drove the initial %desire for a researcher investigation is well worth examining.  Moreover, understanding why Bitcoin Core fuzzing was, temporarily but not %fundamentally, saturated, may be useful to help other fuzzing campaigns avoid the same false saturation problem.

Saturation, as defined in the blog post (\url{https://blog.regehr.org/archives/1796}) that brought Chaincode Labs to the first author, is when ``We apply a fuzzer to some non-trivial system... initially it finds a lot of bugs... [but] the number of new bugs found by the fuzzer drops off, eventually approaching zero.''  That is, at first a particular fuzzer applied to a system will tend to continuously, sometimes impressively, increase both coverage and discovery of previously-unknown bugs.  But, at some point, these bugs are known (and often fixed) and the fuzzer stops producing new bugs.  Code and behavioral coverage seems to be \emph{saturated}.

The underlying reason for saturation is that any fuzzer (or other test generator) explores a space of generated tests according to some, perhaps very complex, probability distribution.  Some bugs lie in the high-probability portion of this space, and other bugs lie in very low probability or even zero probability parts of the space.  

\begin{sloppypar}
  One problem for inexperienced fuzzer users is that it is not always clear just how long a fuzzer needs to run.  Anyone used to popular random testing tools, such as QuickCheck~\cite{ClaessenH00}, may expect a ``reasonable'' budget to be on the order of hundreds of tests or at most a few minutes~\cite{HolmesLOC}.  The fuzz testing research community has settled, to some extent, on 24 hours as a basis for evaluation of fuzzers~\cite{evalfuzz}, but even this may be considered a low-budget run in a serious campaign!  The Solidity compiler fuzzing effort discussed in the saturation blog post (\url{https://blog.trailofbits.com/2020/06/05/breaking-the-solidity-compiler-with-a-fuzzer/}, \url{https://blog.trailofbits.com/2021/03/23/a-year-in-the-life-of-a-compiler-fuzzing-campaign/}) found many bugs only after running a specialized version of AFL for over \emph{a month}.   \emph{Good fuzzing takes a lot of time.}
  \end{sloppypar}

One thing that quickly emerged from discussions with Chaincode before the primary 80 hour effort was the limited extent of the fuzzer runs performed in early April.  The fuzzing includes a large number of targets, each with its own fuzz harness and executable.  At the time, the basic strategy was to run libFuzzer on each of these for 100,000 iterations.  Because some targets are very fast and a few, such as full message processing, are slow, this meant in practice fuzzing most targets for only 30-90 seconds, and even the slowest targets for only a little over an hour.  The total time for over 100 targets was not negligible, but expecting such short runs for each target, after an initial exploration of the easy part of the probability space, to gain coverage or bugs very often, was simply unrealistic.  In particular, for complex, critical targets such as transaction verification and end-to-end message processing, 100,000 iterations was completely insufficient.  When measuring tests created by humans, or even stored by a fuzzer or search-based test generation tool as interesting, 100,000 is a very large number of tests.  When measuring inputs (technically individual tests of a kind) generated during fuzzing or random testing, 100,000 may be a very small number of tests.

The first suggestion for escaping coverage, therefore was very simple:  run the fuzzer longer!  The Chaincode team immediately tried increasing their configuration to 5 million iterations, multiplying the number of executions, and runtime, by a factor of 50.  Based on initial success with a few targets, this was done for all targets, and eventually became the new default.   This exploration of simply increasing the fuzzing budget came early, about 15 days after the initial contact.  The Chaincode team also added new seeds by, as advised, running more fuzzers, including AFL and Honggfuzz, in the same time frame.

By May 20th, Bitcoin Core was also in OSS-Fuzz (it was not at the time of the first discussions, due to reporting requirements, but negotiations settled this problem): \url{https://github.com/google/oss-fuzz/tree/master/projects/bitcoin-core}.  From then on, Bitcoin Core has essentially been continuously fuzzed, and OSS-Fuzz quickly produced new crashes to investigate, and continues to do so:  \url{https://bugs.chromium.org/p/oss-fuzz/issues/list?q=bitcoin}.
